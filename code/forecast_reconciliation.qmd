---
title: "Forecast Reconciliation for SCM"
date: today
format:
  html:
    theme: litera
    embed-resources: true 
    code-tools: true
    toc: true
    toc-location: right
    number-sections: true
    link-external-newwindow: true
    link-external-icon: true
    math: katex
---

# Libraries and Loading Data

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = F, warning = F, fig.align='center', fig.dim=c(10, 6))
```

```{r}
library(tidyverse)
library(sf)
library(viridis)
library(lubridate)
library(tsibble)
library(fable)
library(fabletools)
library(feasts)
library(janitor)
library(rnaturalearth)
library(rnaturalearthdata)

data = read_csv("dynamic_supply_chain_logistics_dataset.csv", show_col_types = FALSE) %>%
  rename(lon = vehicle_gps_longitude, lat = vehicle_gps_latitude) %>%
  mutate(
    lon = suppressWarnings(as.numeric(lon)),
    lat = suppressWarnings(as.numeric(lat)),
    timestamp = lubridate::ymd_hms(timestamp, quiet = TRUE),
    delay_hours = pmax(suppressWarnings(as.numeric(delivery_time_deviation)), 0)
  ) %>%
  filter(!is.na(lon), !is.na(lat), !is.na(timestamp))
```

# Visualisations

We provide delay probabilities overlaid on the US map, and a subset of it on the SoCal region.

```{r}
# Base map
us = ne_states(country = "United States of America",
                              returnclass = "sf")
us_conus = subset(us, !name %in% c("Alaska","Hawaii","Puerto Rico")) |>
  st_transform(4326)

pts = st_as_sf(data, coords = c("lon","lat"), crs = 4326, remove = FALSE)

# Clip points inside
inside = lengths(st_within(pts, us_conus)) > 0
pts_in = pts[inside, , drop = FALSE]

# thin for legibility
set.seed(1)
pts_plot = if (nrow(pts_in) > 80000) pts_in[sample(nrow(pts_in), 80000), ] else pts_in

us_outline = st_cast(us_conus, "MULTILINESTRING")

# Plot delay probability
color_var = "delay_probability"
has_color = color_var %in% names(pts_plot)

p = ggplot() +
  geom_sf(data = us_conus, fill = "grey98", color = "grey90", linewidth = 0.2) +
  { if (has_color)
      geom_point(data = st_drop_geometry(pts_plot),
                 aes(x = lon, y = lat, color = .data[[color_var]]),
                 alpha = 0.35, size = 0.25)
    else
      geom_point(data = st_drop_geometry(pts_plot),
                 aes(x = lon, y = lat),
                 color = "steelblue", alpha = 0.35, size = 0.25)
  } +
  geom_sf(data = us_outline, fill = NA, color = "grey20", linewidth = 0.3) +
  coord_sf(xlim = st_bbox(us_conus)[c("xmin","xmax")],
           ylim = st_bbox(us_conus)[c("ymin","ymax")],
           expand = FALSE, clip = "on") +
  { if (has_color) scale_color_viridis_c(name = color_var, limits = c(0,1)) } +
  labs(title = "Delay Probabilities: All States",
       x = "Longitude", y = "Latitude") +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank())

print(p)
```

## Southern California

```{r}
# California + SoCal bbox with border
# Polygon
ca = rnaturalearth::ne_states(country = "United States of America",
                              returnclass = "sf") |>
  subset(name == "California") |>
  st_transform(4326)

# points as sf 
pts_all = st_as_sf(data, coords = c("lon","lat"), crs = 4326, remove = FALSE)

# only in CA
pts_ca = st_intersection(pts_all, ca)

# crop
socal_bbox = c(xmin = -121, ymin = 32, xmax = -114, ymax = 36)  # note: st_bbox expects xmin,ymin,xmax,ymax
bbox_poly = st_as_sfc(st_bbox(socal_bbox, crs = sf::st_crs(4326)))
pts_socal = st_intersection(pts_ca, bbox_poly)

cat("CA points:", nrow(pts_ca), " | SoCal-in-CA points:", nrow(pts_socal), "\n")

# plot
ca_outline = sf::st_cast(ca, "MULTILINESTRING")

p_socal = ggplot() +
  geom_sf(data = ca, fill = "grey98", color = "grey85", linewidth = 0.2) +
  # bubble layer
  geom_sf(
    data = pts_socal,
    aes(color = delay_probability, size = delay_probability),
    alpha = 0.55
  ) +
  geom_sf(data = ca_outline, fill = NA, color = "grey20", linewidth = 0.4) +
  coord_sf(xlim = c(socal_bbox["xmin"], socal_bbox["xmax"]),
           ylim = c(socal_bbox["ymin"], c(socal_bbox["ymax"])),
           expand = FALSE, clip = "on") +
  scale_color_viridis_c(name = "delay_probability", limits = c(0, 1)) +
  scale_size_continuous(range = c(2, 5), name = "delay_probability", guide = "none") +
  labs(title = "Delay Probabilities: Southern California",
       x = "Longitude", y = "Latitude") +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank())

print(p_socal)
```

# Variables

For reconciliation, we should use an **additive** target variable. Thus, we define a new target based on target D (Delivery Time Deviation) as **total delay time**, given as $$T=\sum \max\{D_i, 0\},\quad D_i=\text{delivery time deviation}.$$For this reason, we add the `delay_time` variable.

```{r}
data = data %>% mutate(delay_time = pmax(delivery_time_deviation, 0))
```

We also add a `State` variable for the two-level hierarchy on `State/cell_id`.

```{r}
# 1) Make fields numeric + additive ingredient (row-level)
data = data %>%
  mutate(
    lon = suppressWarnings(as.numeric(lon)),
    lat = suppressWarnings(as.numeric(lat)),
    timestamp = ymd_hms(timestamp, quiet = TRUE),
    delay_hours = pmax(as.numeric(delivery_time_deviation), 0)
  ) %>%
  filter(!is.na(lon), !is.na(lat), !is.na(timestamp))

# 2) Attach US state (for two-level hierarchy: TOTAL → state → cell_id)
us = ne_states(country = "United States of America", returnclass = "sf")
us_conus = subset(us, !name %in% c("Alaska","Hawaii","Puerto Rico")) %>% st_transform(4326)

pts = st_as_sf(data, coords = c("lon","lat"), crs = 4326, remove = FALSE)
data = st_join(pts, us_conus["name"], left = TRUE) %>% st_drop_geometry()
names(data)[names(data) == "name"] = "state"
data = filter(data, !is.na(state))
```

# Forecast

We first generate the *hierarchical* time series data associated with `data`. We group the data by months as well to get better totals; we tried hourly (default) and daily but don't get any good results.

```{r}
data_ts = data %>% mutate(month = yearmonth(timestamp),
                       cargo = if_else(cargo_condition_status <= 0.5, "Poor", "Good")) %>%
  arrange(month, state, cargo, timestamp) %>%
  distinct(month, state, cargo, .keep_all = TRUE)

ts = data_ts %>% as_tsibble(key = c(state, cargo), index = month) %>%
  aggregate_key(state/cargo, delay_hours = sum(delay_hours))
```

## Train-Test Split

```{r}
n_test = 3  # last 3 months
cutoff = max(ts$month, na.rm = TRUE) - n_test

train_data = ts %>% filter(month <= cutoff)
test_data  = ts %>% filter(month >  cutoff)
```

When we try to fit an ETS model, we generally get NULL models for a lot of these entries, so we counteract this by doing the specified ETS model for the non-NULL models, and set the rest as a TSLM model.

- ETS (additive) captures level/trend/season for non-negative flows and adapts well when there’s actual signal.
- TSLM(~1) (intercept-only) is a safe fallback for all-zero or ultra-sparse series; it yields a valid (typically zero) forecast instead of a <NULL model>, keeping the hierarchy intact for reconciliation.

```{r}
# Fallback if null
fb = train_data %>% model(model = TSLM(delay_hours ~ 1))

# main fitting
fit = train_data %>% model(model = ETS(delay_hours ~ error("A")))
is_null = is_null_model(fit$model)
fit$model[is_null] = fb$model[is_null]
```

## Reconciliation

```{r}
recon_fc = fit %>%
  reconcile(BU = bottom_up(model), 
            OLS = min_trace(model, "ols"), 
            WLS = min_trace(model, "wls_struct")) %>% 
  forecast(h = n_test)
```

### Diagnostics

```{r}
recon_tibble = recon_fc %>% as_tibble()

first_month = recon_tibble %>% 
  summarise(m1 = min(month)) %>% 
  pull(m1)

total_m1 = recon_tibble %>% 
  filter(month == first_month, is_aggregated(state), is_aggregated(cargo)) %>%
  select(.model, total = .mean)
```

